# -*- coding: utf-8 -*-
"""textmine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vYEhOo8Mbo6VOfaX_YN6F0G6S3eL7tY4
"""

pip install pyLDAvis

import nltk
nltk.download('stopwords')

from google.colab import drive

drive.mount('/gdrive')
gdrive_root = '/gdrive/My Drive/Colab Notebooks' # gdrive_root + '/data_TM/koreaherald_1517_5.json

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

import re
import numpy as np
import pandas as pd
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

file_list = [gdrive_root + '/Data/koreaherald_1517_' + str(i) + '.json' for i in range(5, 8)]
# data = pd.read_json(gdrive_root + '/data_TM/koreaherald_1517_5.json')
data_body = []
data_title = []
data_time = []
import os
print(file_list[0])
for i, file in enumerate(file_list):
    df = pd.read_json(file)
    df_body = df[' body'].values.tolist()
    df_title = df['title'].values.tolist()
    df_time = df[' time'].values.tolist()
    if i == 0:
      data_body += df_body[1614:]
      data_title += df_title[1614:]
      data_time += df_time[1614:]
    else:
      data_body += df_body
      data_title += df_title
      data_time += df_time

data = data_body

#pre-processing
def remove_hyphen(data):
    data = [re.sub('-', '', sent) for sent in data]
    data = [re.sub(r'[\w\.-]+@[\w\.-]+', '', sent) for sent in data]
    return data

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))
    return sentence

data = remove_hyphen(data_body)
data_words = list(sent_to_words(data))

bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50)  # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=50)
quadragram = gensim.models.Phrases(trigram[bigram[data_words]], threshold=50)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
quadragram_mod = gensim.models.phrases.Phraser(quadragram)

nlp = spacy.load('en', disable=['parser', 'ner'])

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_quadragrams(texts):
    return [quadragram_mod[trigram_mod[bigram_mod[doc]]] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out


### lemmatization with NLTK ###
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

lemmatizer = WordNetLemmatizer()

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def lemmatization_nltk_wnl(text):
  text_out = []
  for i in range(len(text)):
    word_set = []
    for word in text[i]:
      word_set.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))
    text_out.append(word_set)
  return text_out

data_words_nostops = remove_stopwords(data_words)
#data_lemmatized = lemmatization_nltk_wnl(data_words_nostops)
# Create Dictionary
#id2word = corpora.Dictionary(data_lemmatized)
id2word = corpora.Dictionary(data_words_nostops)
# Create Corpus
#texts = data_lemmatized
texts = data_words_nostops
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]


import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize(text):
    print(text)
    tokens = nltk.word_tokenize(text)
    stems = stem_tokens(tokens, stemmer)
    print(stems)
    return stems

def LemmaTokenizer(text):
    a = []
    a.append(text)
    data = remove_hyphen(a)
    data_words = sent_to_words(data)
    data_words_nostops = remove_stopwords(data_words)
    data_words_bigrams = make_bigrams(data_words_nostops)
    data_lemmatized = lemmatization_nltk_wnl(data_words_bigrams)
    return data_lemmatized[0]


count_vect = CountVectorizer(tokenizer=LemmaTokenizer, max_df=0.8, min_df=2, stop_words='english')
doc_term_matrix = count_vect.fit_transform(data)

LDA = LatentDirichletAllocation(n_components=26, random_state=42)
LDA.fit(doc_term_matrix)

import random

first_topic = LDA.components_[0]
top_topic_words = first_topic.argsort()[-10:]

for i,topic in enumerate(LDA.components_):
    print(f'Top 10 words for topic #{i}:')
    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')

lda_output = LDA.transform(doc_term_matrix)
topicnames = ["Topic" + str(i) for i in range(LDA.n_components)]
docnames = ["Doc" + str(i) for i in range(len(data))]
df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)
dominant_topic = np.argmax(df_document_topic.values, axis=1)
df_document_topic['dominant_topic'] = dominant_topic
# Styling
def color_green(val):
    color = 'green' if val > .1 else 'black'
    return 'color: {col}'.format(col=color)

def make_bold(val):
    weight = 700 if val > .1 else 400
    return 'font-weight: {weight}'.format(weight=weight)

# Apply Style
df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)
df_document_topics

df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name="Num Documents")
df_topic_distribution.columns = ['Topic Num', 'Num Documents']
top10_topic = []
for i in range (0, 10) :
  top10_topic.append(df_topic_distribution['Topic Num'][i])
top10_topic
df_topic_distribution

for i in range (0, LDA.n_components) : 
  topic_idx = 'Topic' + str(i)
  tmp = list(df_document_topic[topic_idx])
  cnt = 0
  print(i)
  while cnt < 3 and tmp: 
    ind = tmp.index(max(tmp))
    if df_document_topic['dominant_topic'][ind] == i :
      print(data_title[tmp.index(max(tmp))])
      print(data_body[tmp.index(max(tmp))])
      cnt = cnt + 1
    tmp.remove(max(tmp))
  print([count_vect.get_feature_names()[j] for j in LDA.components_[i].argsort()[-20:]])
  print("\n")

for i in range (0, len(top10_topic)) : 
  topic_idx = 'Topic' + str(top10_topic[i])
  tmp = list(df_document_topic[topic_idx])
  cnt = 0
  print(top10_topic[i])
  while cnt < 3 : 
    ind = tmp.index(max(tmp))
    if df_document_topic['dominant_topic'][ind] == top10_topic[i] :
      print(data_title[tmp.index(max(tmp))])
      cnt = cnt + 1
    tmp.remove(max(tmp))
  print([count_vect.get_feature_names()[j] for j in LDA.components_[i].argsort()[-20:]])
  print("\n")

from nltk import ConditionalFreqDist
from nltk.util import ngrams
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.cluster import DBSCAN
from nltk.probability import ConditionalProbDist, MLEProbDist
import nltk
nltk.download('punkt')

def make_df(topic_num) :
  titles = []
  dates = []

  for i in range (0, len(data_title)) :
    if df_document_topic['dominant_topic'][i] == topic_num :
      titles.append(data_title[i])
      dates.append(data_time[i])

  df_ = pd.DataFrame({'title' : titles, 'date' : dates})
  df_ = df_.drop_duplicates(subset='title').reset_index(drop=True)
  df_ = df_.dropna()

  df_.head()
  return df_

import spacy.cli
spacy.cli.download("en_core_web_lg")

pip install tqdm

from tqdm import tqdm

nlp = spacy.load('en_core_web_lg')

def make_vectors(df_) :
  sent_vecs = {}
  docs = []

  for title in tqdm(df_.title) :
    doc = nlp(title)
    docs.append(doc)
    sent_vecs.update({title : doc.vector})

  sentences = list(sent_vecs.keys())
  vectors = list(sent_vecs.values())
  return sentences, vectors

def make_dbscan(vectors) :

  x = np.array(vectors)

  n_classes = {}
  clu_size = []
  eps_val = []
  for i in tqdm(np.arange(0.001, 1, 0.002)) :
    dbscan = DBSCAN(eps=i, min_samples=5, metric='cosine').fit(x)
    n_classes.update({i: len(pd.Series(dbscan.labels_).value_counts())})
    eps_val.append(i)
    clu_size.append(n_classes[i])
  #print(n_classes)
  #print(eps_val[clu_size.index(max(clu_size))])
  max_eps = eps_val[clu_size.index(max(clu_size))]
  print(clu_size)
  dbscan = DBSCAN(eps=max_eps, min_samples=5, metric='cosine').fit(x)
  labels = dbscan.fit_predict(x)
  print("클러스터 수: {}".format(len(np.unique(labels))))
  print("클러스터 크기: {}".format(np.bincount(labels + 1)))

  return dbscan, labels

def get_mean_vector(sents) : 
  a = np.zeros(300)
  for sent in sents :
    a = a + nlp(sent).vector
  return a/len(sents)

def get_central_vector(sents):
  vecs = []
  for sent in sents : 
    doc = nlp(sent)
    vecs.append(doc.vector)
  mean_vec = get_mean_vector(sents)
  index = pairwise_distances_argmin_min(np.array([mean_vec]), vecs)[0][0]
  return sents[index]

def make_cluster(dbscan, labels, sentences, df_) :
  results = pd.DataFrame({'label' : dbscan.labels_, 'sent' : sentences})
  num_cluster = []
  for k in range (0, len(np.unique(labels))-1) :
    print("i is : " + str(k))
    example_result = results[results.label == k].sent.tolist()
    event_df = df_[df_.title.isin(example_result)][['date', 'title']]
    event_df['date'] = pd.to_datetime(event_df.date)
    event_df = event_df.sort_values(by='date').dropna()
    t = event_df['title'].tolist()
    d = event_df['date'].tolist()

    res_t = []
    res_d = []
    i = 0
    while i < len(t) :
      year = d[i].year
      month = d[i].month
      day = d[i].day
      sents = []
      last = 0
      for j in range (i, len(t)) :
        if d[j].year == year and d[j].month == month and d[j].day == day :
          sents.append(t[j])
          last = j
      ind = t.index(get_central_vector(sents))
      res_t.append(t[ind])
      res_d.append(d[ind])
      if last != 0 :
        i = last
      i = i+1

    
    for i in range (0, len(res_t)) :
      print(res_d[i].strftime('%y-%m-%d %H:%M:%S') + "  " + res_t[i])
    if k != -1 :
      num_cluster.append(len(res_t))
  return num_cluster, results

def make_token(results, num_cluster, df_) :
  max_cluster_ind = num_cluster.index(max(num_cluster))

  example_result = results[results.label == max_cluster_ind].sent.tolist()
  event_df = df_[df_.title.isin(example_result)][['date', 'title']]
  event_df['date'] = pd.to_datetime(event_df.date)
  event_df = event_df.sort_values(by='date').dropna()
  t = event_df['title'].tolist()
  tot = 0

  for i in range (0, len(t)) :
    t[i] = word_tokenize(t[i])
    if 'S.' in t[i] :
      ind = t[i].index('S.')
      t[i][ind] = 'S.Korea'
      t[i].remove(t[i][ind+1])
    if 'N.' in t[i] :
      ind = t[i].index('N.')
      t[i][ind] = 'N.Korea'
      t[i].remove(t[i][ind+1])
    tot = tot + len(t[i])
    t[i].append(".")
  #print(t)
  #print(tot/len(t))
  avg = int(tot/len(t)*2/3.0)
  #print(avg) 


  sentence = []
  for tokens in t:
    bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol="SS", right_pad_symbol="SE")
    sentence += [t for t in bigram]

  cfd = ConditionalFreqDist(sentence)
  cpd = ConditionalProbDist(cfd, MLEProbDist)
  return cpd, avg

def sentence_score(s,cpd):
  p = 0.0
  for i in range(len(s) - 1):
      c = s[i]
      w = s[i + 1]
      p += np.log(cpd[c].prob(w) + np.finfo(float).eps)
  return np.exp(p)

def generate_sentence(seed, cpd):
    if seed is not None:
        import random
        random.seed(seed)
    c = "SS"
    sentence = []
    while True:
        if c not in cpd:
            break
        w = cpd[c].generate()

        if w == "SE":
            break
        elif w in ["i", "ii", "iii"]:
            w2 = w.upper()
        else:
            w2 = w

        if c == "SS":
            sentence.append(w2.title())
        elif c in ["`", "\"", "'", "("]:
            sentence.append(w2)
        elif w in ["'", ".", ",", ")", ":", ";", "?"]:
            sentence.append(w2)
        else:
            sentence.append(" " + w2)

        c = w
    return "".join(sentence)

def trending_list(topic_num) :
  df_ = make_df(topic_num) 
  sentences, vectors = make_vectors(df_)
  dbscan, labels = make_dbscan(vectors)
  num_cluster, results = make_cluster(dbscan, labels, sentences, df_)
  cpd, avg = make_token(results, num_cluster, df_)
  
  score = []
  sen_ind = []
  sent = []
  for i in range (0, 50) :
    #print(generate_sentence(i))
    if len(word_tokenize(generate_sentence(i, cpd))) > avg:
      #print(len(word_tokenize(generate_sentence(i))))
      score.append(sentence_score(word_tokenize(generate_sentence(i, cpd)),cpd))
      sen_ind.append(i)
      #print(generate_sentence(i))
      #print(sentence_score(word_tokenize(generate_sentence(i))))

  for i in range (0, 10) :
    m = max(score)
    ind = score.index(m)
    sent.append(generate_sentence(sen_ind[ind], cpd))
    score[ind] = 0
  return sent, get_central_vector(sent), results, df_, num_cluster.index(max(num_cluster))

central = []
results_set = []
df_set = []
max_clu = []
for i in range (0, 10) :
  sent, cent, results, df_, clu_num = trending_list(int(top10_topic[i]))
  results_set.append(results)
  df_set.append(df_)
  max_clu.append(clu_num)
  #print(sent)
  central.append(cent)
for i in range (0, 10) :
  print(central[i])

example_result = results_set[0][results_set[0].label == 0].sent.tolist()
event_df = df_set[0][df_set[0].title.isin(example_result)][['date', 'title']]
event_df['date'] = pd.to_datetime(event_df.date)
event_df = event_df.sort_values(by='date').dropna()
t = event_df['title'].tolist()

print(t)
tot = 0
sent_vecs = {}
docs = []
date_list = []
title_list = []
for date in tqdm(df_set[0].date) :
  a = date.split(" ")
  b = a[0].split("-")
  val = 30 * (int(b[1])-1) + int(b[2])
  date_list.insert(0,val)

for title in tqdm(df_set[0].title) :
  title_list.insert(0, title)

x = []
for i in range (0, len(date_list)) :
  x.append([int(date_list[i]), title_list[i]])
print(x)